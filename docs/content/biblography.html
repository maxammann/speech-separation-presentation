<section>
    <h2>References</h2>
    <div id="refs" class="references" role="doc-bibliography">
        <div id="ref-Hershey2015">
            <p>[1] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep clustering: Discriminative embeddings for
                segmentation and separation,” 2015.</p>
        </div>
        <div id="ref-Huang2014">
            <p>[2] P. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis, “Deep learning for monaural speech
                separation,” in <em>IEEE international conferenceon acoustics, speech and signal processing
                    (icassp)</em>, 2014, pp. 1562–1566.</p>
        </div>
        <div id="ref-Wang2018b">
            <p>[3] D. Wang and J. Chen, “Supervised speech separation based on deep learning: An overview,” <em>IEEE/ACM
                Transactions on Acoustics, Speech, and Signal Processing</em>, vol. 26, no. 10, pp. 1702–1726, Oct.
                2018.</p>
        </div>
        <div id="ref-Cherry1953">
            <p>[4] E. C. Cherry, “Some experiments on the recognition of speech, with one and with two ears,” <em>The
                Journal of the Acoustical Society of America</em>, vol. 25, no. 5, pp. 975–979, 1953.</p>
        </div>
        <div id="ref-Virtanen2007">
            <p>[5] T. Virtanen, “Monaural sound source separation by nonnegative matrix factorization with temporal
                continuity and sparseness criteria,” <em>IEEE Transactions on Audio, Speech and Language Processing</em>,
                vol. 15, no. 3, pp. 1066–1074, Mar. 2007.</p>
        </div>
        <div id="ref-Vincent2006">
            <p>[6] E. Vincent, R. Gribonval, and C. Fevotte, “Performance measurement in blind audio source separation,”
                <em>IEEE Transactions on Audio, Speech and Language Processing</em>, vol. 14, no. 4, pp. 1462–1469, Jul.
                2006.</p>
        </div>
        <div id="ref-Wang2018">
            <p>[7] Z.-Q. Wang, J. L. Roux, and J. R. Hershey, “Alternative objective functions for deep clustering,” in
                <em>IEEE international conferenceon acoustics, speech and signal processing (icassp)</em>, 2018.</p>
        </div>
        <div id="ref-Isik2016">
            <p>[8] Y. Isik, J. L. Roux, Z. Chen, S. Watanabe, and J. R. Hershey, “Single-channel multi-speaker
                separation using deep clustering,” in <em>Interspeech</em>, 2016.</p>
        </div>
        <div id="ref-Stoeter2018">
            <p>[9] F.-R. Stöter, A. Liutkus, and N. Ito, “The 2018 Signal Separation Evaluation Campaign,” Apr.
                2018.</p>
        </div>
        <div id="ref-Adiga2016">
            <p>[10] M. T. Adiga and R. Bhandarkar, “Improving single frequency filtering based voice activity detection
                (VAD) using spectral subtraction based noise cancellation,” in <em>2016 international conference on
                    signal processing, communication, power and embedded system (SCOPES)</em>, 2016.</p>
        </div>
    </div>
</section>

<section>
    <div class="references" role="doc-bibliography">
        <div id="ref-Hershey2016">
            <p>[11] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep clustering: Discriminative embeddings for
                segmentation and separation,” in <em>IEEE international conferenceon acoustics, speech and signal
                    processing (icassp)</em>, 2016.</p>
        </div>
        <div id="ref-Lyons1997">
            <p>[12] R. G. Lyons, <em>Understanding digital signal processing</em>, 1st ed. Addison Wesley Pub. Co, 1997.
            </p>
        </div>
        <div id="ref-Cooley1965">
            <p>[13] J. W. Cooley and J. W. Tukey, “An algorithm for the machine calculation of complex fourier series,”
                <em>Mathematics of Computation</em>, vol. 19, no. 90, pp. 297–297, May 1965.</p>
        </div>
        <div id="ref-Emiya2011">
            <p>[14] V. Emiya, E. Vincent, N. Harlander, and V. Hohmann, “Subjective and objective quality assessment of
                audio source separation,” <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, vol. 19,
                no. 7, pp. 2046–2057, Sep. 2011.</p>
        </div>
        <div id="ref-Oppenheim1999">
            <p>[15] A. V. Oppenheim, R. W. Schafer, and J. R. Buck, <em>Discrete-time signal processing</em>, 2nd ed.
                Prentice-Hall, Inc., 1999.</p>
        </div>
        <div id="ref-Smith2011">
            <p>[16] J. O. Smith, “Overlap-add stft processing,” in <em>Spectral audio signal processing</em>, 2011.</p>
        </div>
        <div id="ref-Heinzel2002">
            <p>[17] G. Heinzel, A. Ruediger, and R. Schilling, “Spectrum and spectral density estimation by the Discrete
                Fourier transform (DFT), including a comprehensive list of window functions and some new at-top
                windows.” Feb-2002.</p>
        </div>
        <div id="ref-Oppenheim2014">
            <p>[18] A. V. Oppenheim and R. W. Schafer, <em>Discrete-time signal processing</em>, 3rd ed. Pearson
                Education, 2014.</p>
        </div>
        <div id="ref-Oliphant2006">
            <p>[19] T. E. Oliphant, <em>A guide to numpy</em>, vol. 1. Trelgol Publishing, 2006.</p>
        </div>
        <div id="ref-Harris1978">
            <p>[20] F. J. Harris, “On the use of windows for harmonic analysis with the discrete fourier transform,”
                <em>Proceedings of the IEEE</em>, vol. 66, no. 1, pp. 51–83, 1978.</p>
        </div>
        <div id="ref-Rosen2013">
            <p>[21] S. Rosen, P. Souza, C. Ekelund, and A. A. Majeed, “Listening to speech in a background of other
                talkers: Effects of talker number and noise vocoding,” <em>The Journal of the Acoustical Society of
                    America</em>, vol. 133, no. 4, pp. 2431–2443, Apr. 2013.</p>
        </div>
        <div id="ref-Miller1947">
            <p>[22] G. A. Miller, “The masking of speech.” <em>Psychological Bulletin</em>, vol. 44, no. 2, pp. 105–129,
                1947.</p>
        </div>
        <div id="ref-Wang2006">
            <p>[23] D. L. Wang and G. J. Brown, <em>Computational auditory scene analysis: Principles, algorithms, and
                applications</em>. Wiley, 2006.</p>
        </div>
        <div id="ref-Festen1990">
            <p>[24] J. M. Festen and R. Plomp, “Effects of fluctuating noise and interfering speech on the
                speech-reception threshold for impaired and normal hearing,” <em>The Journal of the Acoustical Society
                    of America</em>, vol. 88, no. 4, pp. 1725–1736, Oct. 1990.</p>
        </div>
        <div id="ref-Lee1999">
            <p>[25] D. D. Lee and H. S. Seung, “Learning the parts of objects by non-negative matrix factorization,”
                <em>Nature</em>, vol. 401, no. 6755, pp. 788–791, Oct. 1999.</p>
        </div>
        <div id="ref-Dubnov2002">
            <p>[26] S. Dubnov, “Extracting sound objects by independent subspace analysis.” Jun-2002.</p>
        </div>
        <div id="ref-Casey2000">
            <p>[27] M. A. Casey and A. Westner, “Separation of mixed audio sources by independent subspace analysis,” in
                <em>ICMC</em>, 2000.</p>
        </div>
        <div id="ref-Wang2005">
            <p>[28] D. Wang, “On Ideal Binary Mask As the Computational Goal of Auditory Scene Analysis,” in <em>Speech
                separation by humans and machines</em>, P. Divenyi, Ed. Springer, 2005, pp. 181–197.</p>
        </div>
        <div id="ref-Schuller2010">
            <p>[29] B. Schuller, F. Weninger, M. Wollmer, Y. Sun, and G. Rigoll, “Non-negative matrix factorization as
                noise-robust feature extractor for speech recognition,” in <em>IEEE international conferenceon
                    acoustics, speech and signal processing (icassp)</em>, 2010.</p>
        </div>
        <div id="ref-Qian2018">
            <p>[30] Y.-m. Qian, C. Weng, X.-k. Chang, S. Wang, and D. Yu, “Past review, current progress, and challenges
                ahead on the cocktail party problem,” <em>Frontiers of Information Technology &amp; Electronic
                    Engineering</em>, vol. 19, no. 1, pp. 40–63, Jan. 2018.</p>
        </div>
        <div id="ref-Hu2007">
            <p>[31] Y. Hu and P. C. Loizou, “Subjective comparison and evaluation of speech enhancement algorithms,”
                <em>Speech Communication</em>, vol. 49, nos. 7-8, pp. 588–601, Jul. 2007.</p>
        </div>
        <div id="ref-Ephraim1985">
            <p>[32] Y. Ephraim and D. Malah, “Speech enhancement using a minimum mean-square error log-spectral
                amplitude estimator,” <em>IEEE Transactions on Acoustics, Speech, and Signal Processing</em>, vol. 33,
                no. 2, pp. 443–445, Apr. 1985.</p>
        </div>
        <div id="ref-Marr1982">
            <p>[33] D. Marr, <em>Vision: A computational investigation into the human representation and processing of
                visual information</em>. Henry Holt; Co., Inc., 1982.</p>
        </div>
        <div id="ref-Schmidt2006">
            <p>[34] M. N. Schmidt and R. K. Olsson, “Single-channel speech separation using sparse non-negative matrix
                factorization,” in <em>Interspeech</em>, 2006.</p>
        </div>
        <div id="ref-Tu2014">
            <p>[35] Y. Tu, J. Du, Y. Xu, L. Dai, and C.-H. Lee, “Deep neural network based speech separation for robust
                speech recognition,” in <em>12th ieee international conference on signal processing (ICSP)</em>, 2014.
            </p>
        </div>
        <div id="ref-Arthur2007">
            <p>[36] D. Arthur and S. Vassilvitskii, “K-means++: The advantages of careful seeding,” in <em>Proceedings
                of the eighteenth annual acm-siam symposium on discrete algorithms</em>, 2007, pp. 1027–1035.</p>
        </div>
        <div id="ref-Lloyd1982">
            <p>[37] S. Lloyd, “Least squares quantization in PCM,” <em>IEEE Transactions on Information Theory</em>,
                vol. 28, no. 2, pp. 129–137, Mar. 1982.</p>
        </div>
        <div id="ref-Berkhin2002">
            <p>[38] P. Berkhin, “Survey of clustering data mining techniques,” 2002.</p>
        </div>
        <div id="ref-Paliwal2011">
            <p>[39] K. Paliwal, K. Wójcicki, and B. Shannon, “The importance of phase in speech enhancement,” <em>Speech
                Communication</em>, vol. 53, no. 4, pp. 465–494, Apr. 2011.</p>
        </div>
        <div id="ref-Williamson2016">
            <p>[40] D. S. Williamson, Y. Wang, and D. Wang, “Complex ratio masking for monaural speech separation,” <em>IEEE/ACM
                Transactions on Audio, Speech, and Language Processing</em>, vol. 24, no. 3, pp. 483–492, Mar. 2016.</p>
        </div>
        <div id="ref-Goodfellow2016">
            <p>[41] I. Goodfellow, Y. Bengio, and A. Courville, <em>Deep learning</em>. MIT Press, 2016.</p>
        </div>
        <div id="ref-Maas2013">
            <p>[42] A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectifier nonlinearities improve neural network acoustic
                models,” in <em>Proceedings of the international conference on machine learning</em>, 2013.</p>
        </div>
        <div id="ref-Nielsen2015">
            <p>[43] M. A. Nielsen, <em>Neural networks and deep learning</em>. Determination Press, 2015.</p>
        </div>
        <div id="ref-Graves2012">
            <p>[44] A. Graves, <em>Supervised sequence labelling with recurrent neural networks</em>. Springer, 2012.
            </p>
        </div>
        <div id="ref-Schuster1997">
            <p>[45] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,” <em>IEEE Transactions on
                Signal Processing</em>, vol. 45, no. 11, pp. 2673–2681, 1997.</p>
        </div>
        <div id="ref-Williams1995">
            <p>[46] R. J. Williams and D. Zipser, “Gradient-Based Learning Algorithms for Recurrent Networks and Their
                Computational Complexity.” 1995.</p>
        </div>
        <div id="ref-Heide1998">
            <p>[47] D. A. Heide and G. S. Kang, “Speech enhancement for bandlimited speech,” in <em>IEEE international
                conferenceon acoustics, speech and signal processing (icassp)</em>, 1998, vol. 1, pp. 393–396.</p>
        </div>
        <div id="ref-Aloise2009">
            <p>[48] D. Aloise, A. Deshpande, P. Hansen, and P. Popat, “NP-hardness of Euclidean sum-of-squares
                clustering,” <em>Machine Learning</em>, vol. 75, no. 2, pp. 245–248, Jan. 2009.</p>
        </div>
        <div id="ref-Kolbaek2017">
            <p>[49] M. Kolbaek, D. Yu, Z.-H. Tan, and J. Jensen, “Multitalker speech separation with utterance-level
                permutation invariant training of deep recurrent neural networks,” <em>IEEE/ACM Transactions on Audio,
                    Speech, and Language Processing</em>, vol. 25, no. 10, pp. 1901–1913, Oct. 2017.</p>
        </div>
        <div id="ref-Bauckhage2015">
            <p>[50] C. Bauckhage, “k-Means Clustering Is Matrix Factorization,” Dec. 2015.</p>
        </div>
        <div id="ref-Keogh2017">
            <p>[51] E. Keogh and A. Mueen, “Curse of dimensionality,” in <em>Encyclopedia of machine learning and data
                mining</em>, Springer US, 2017, pp. 314–315.</p>
        </div>
        <div id="ref-Pudil1998">
            <p>[52] P. Pudil and J. Novovičová, “Novel methods for feature subset selection with respect to problem
                knowledge,” in <em>Feature extraction, construction and selection</em>, Springer, 1998, pp. 101–116.</p>
        </div>
        <div id="ref-Shlens2014">
            <p>[53] J. Shlens, “A tutorial on principal component analysis,” 2014.</p>
        </div>
        <div id="ref-Garofolo1993">
            <p>[54] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D. S. Pallett, “DARPA TIMIT -
                Acoustic-Phonetic Continuous Speech Corpus. NIST Speech Disc 1-1.1,” vol. 93. Feb-1993.</p>
        </div>
        <div id="ref-Garofolo1994">
            <p>[55] J. S. Garofolo, D. Graff, D. Paul, and D. S. Pallett, “ARPA CSR-WSJ0 - Continuous Speech Recognition
                Wall Street Journal. NIST Speech Discs 11-1.1 - 11-12.1, 11-13.1, 11-14.1, and 11-15.1.” 1994.</p>
        </div>
        <div id="ref-Hernandez2018">
            <p>[56] F. Hernandez, V. Nguyen, S. Ghannay, N. Tomashenko, and Y. Estève, “TED-lium 3: Twice as much data
                and corpus repartition for experiments on speaker adaptation,” in <em>Speech and computer</em>, 2018,
                pp. 198–208.</p>
        </div>
        <div id="ref-Rafii2018">
            <p>[57] Z. Rafii, A. Liutkus, F.-R. Stoter, S. I. Mimilakis, D. FitzGerald, and B. Pardo, “An overview of
                lead and accompaniment separation in music,” <em>IEEE/ACM Transactions on Audio, Speech, and Language
                    Processing</em>, vol. 26, no. 8, pp. 1307–1335, Aug. 2018.</p>
        </div>
        <div id="ref-Kreyszig2006">
            <p>[58] E. Kreyszig, <em>Advanced engineering mathematics</em>, 9th ed. Wiley, 2006.</p>
        </div>
        <div id="ref-Sharpe2018">
            <p>[59] B. Sharpe, “Invertibility of overlap-add processing.” 2018.</p>
        </div>
        <div id="ref-Griffin1984">
            <p>[60] D. Griffin and J. Lim, “Signal estimation from modified short-time fourier transform,” <em>IEEE
                Transactions on Acoustics, Speech, and Signal Processing</em>, vol. 32, no. 2, pp. 236–243, Apr. 1984.
            </p>
        </div>
        <div id="ref-McFee2019">
            <p>[61] B. McFee <em>et al.</em>, “Librosa/librosa: 0.6.3.” Zenodo, 2019.</p>
        </div>
        <div id="ref-Greff2017">
            <p>[62] K. Greff, R. K. Srivastava, J. Koutnik, B. R. Steunebrink, and J. Schmidhuber, “LSTM: A search space
                odyssey,” <em>IEEE Transactions on Neural Networks and Learning Systems</em>, vol. 28, no. 10, pp.
                2222–2232, Oct. 2017.</p>
        </div>
        <div id="ref-Kingma2014">
            <p>[63] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,” <em>arXiv e-prints</em>, p.
                arXiv:1412.6980, Dec. 2014.</p>
        </div>
        <div id="ref-PSF">
            <p>[64] Python Software Foundation, “The python language reference, version 3.7.3.”.</p>
        </div>
        <div id="ref-Pedregosa2011">
            <p>[65] F. Pedregosa <em>et al.</em>, “Scikit-learn: Machine learning in Python,” <em>Journal of Machine
                Learning Research</em>, vol. 12, pp. 2825–2830, 2011.</p>
        </div>
        <div id="ref-Hunter2007">
            <p>[66] J. D. Hunter, “Matplotlib: A 2D graphics environment,” <em>Computing in Science &amp;
                Engineering</em>, vol. 9, no. 3, pp. 90–95, 2007.</p>
        </div>
        <div id="ref-Raffel2014">
            <p>[67] C. Raffel <em>et al.</em>, “mir_eval: A Transparent Implementation of Common MIR Metrics,” <em>Proceedings
                of the 15th International Conference on Music Information Retrieval</em>, 2014.</p>
        </div>
        <div id="ref-Fevotte2005">
            <p>[68] C. Févotte, R. Gribonval, and E. Vincent, “BSS_EVAL Toolbox User Guide – Revision 2.0,” Technical
                Report, 2005.</p>
        </div>
        <div id="ref-Abadi2016">
            <p>[69] M. Abadi <em>et al.</em>, “TensorFlow: A system for large-scale machine learning,” in <em>12th
                usenix symposium on operating systems design and implementation (osdi 16)</em>, 2016, pp. 265–283.</p>
        </div>
        <div id="ref-Vincent2012">
            <p>[70] E. Vincent <em>et al.</em>, “The Signal Separation Evaluation Campaign (20072010): Achievements and
                Remaining Challenges,” <em>Signal Processing</em>, vol. 92, no. 8, pp. 1928–1936, Aug. 2012.</p>
        </div>
        <div id="ref-Vincent2007">
            <p>[71] E. Vincent, H. Sawada, P. Bofill, S. Makino, and J. P. Rosca, “First Stereo Audio Source Separation
                Evaluation Campaign: Data, Algorithms and Results,” in <em>Independent component analysis and signal
                    separation</em>, 2007, pp. 552–559.</p>
        </div>
        <div id="ref-Taal2010">
            <p>[72] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, “A short-time objective intelligibility
                measure for time-frequency weighted noisy speech,” in <em>IEEE international conferenceon acoustics,
                    speech and signal processing (icassp)</em>, 2010.</p>
        </div>
        <div id="ref-Rix2001">
            <p>[73] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, “Perceptual evaluation of speech
                quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs,” in <em>IEEE
                    international conferenceon acoustics, speech and signal processing (icassp)</em>, 2001.</p>
        </div>
        <div id="ref-Chung2014">
            <p>[74] J. Chung, Çaglar Gülçehre, K. Cho, and Y. Bengio, “Empirical evaluation of gated recurrent neural
                networks on sequence modeling,” Dec. 2014.</p>
        </div>
    </div>
</section>
